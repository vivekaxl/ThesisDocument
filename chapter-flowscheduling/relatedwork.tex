\section{Related Work}
\label{sec:related_work}

\subsection{The decoupled model}

In previous work \cite{ShaferJ2010_PhD}, the author compared the performance of Hadoop integrated with different types of storage architecture.
The author found that in the split architecture, Hadoop has imbalance issue access to remote data storage, which can lead to poor I/O performance.
In the following, we discussed the use cases that integrate Hadoop with separate storage.
After that, we describe scheduling methods for Hadoop and cluster that are related to our work.

\subsubsection{Parallel File System}
Several research studies show their interests in replacing HDFS with other high performance storage systems.  
W. Tantisiriroj et al \cite{TantisirirojWittawat2011Duality} argues that parallel file systems can support diverse workloads and provides a better tradeoff between performance and reliability.
The authors proposed a PVFS shim layer to incorporate data layout of PVFS to achieve data locality.
Maltzahn et al considered Ceph as a scalable alternative to HDFS \cite{MaltzahnC2010_Ceph}, and they create a mapping layer which is similar to  the PVFS one.
GPFS is a shared-disked file system developed by IBM, and widely adopted in supercomputers. 
R. Ananthanarayanan et al \cite{AnanthanarayananR2009_CloudAnalytics} from IBM Research modify data layout in GPFS and expose this information to Hadoop.
These are the earliest studies that tried to replace HDFS, but none of them considers optimizing Hadoop at the job scheduler level.

\subsubsection{Enterprise Storage}

Another research study \cite{PorterG2010_SuperDataNodes} analyzed the feasibility to use a very powerful storage node to accommodate Hadoop, and he finds that Hadoop performance is dominated by the bandwidth between computing and storage facilities.

Recently, the researchers in NetApp Inc. argued that it is required to decouple compute and storage nodes in big data analytics because enterprise IT often deploys \textit{silos} to manage high-value data \cite{MihailescuM2012_MixApart}.
The decoupled Hadoop model would incur high cost on data loading from the backend storage system to compute nodes.
They propose MixApart, which includes the data-aware task scheduler, the task-aware data scheduler and a caching mechanism, to optimize Hadoop performance.
However, they mainly focused on highly data-reuse workload.

\subsubsection{Cloud Storage Service}

Cloud computing has emerged as an important technology for the pay-as-you-go model \cite{ArmbrustM2010_CloudComputing, KarthikK2009_HadoopProvisioning}.
In such a platform, object storage, e.g. Amazon S3 and Azure Blob Storage, is primarily chosen for persistent data.
Amazon Elastic MapReduce and Azure HDInsight are two popular Hadoop platforms on the cloud \cite{AWS, WindowsAzure}.
Both cases enable Hadoop to support data access to object storage.

\subsection{Resource and Job Scheduling}

Hadoop has the builtin FIFO scheduler, which allocates resources based on first-come-first-serve policy and data locality.
The Fair Scheduler was originally developed by Facebook with the objective of resource sharing, and Yahoo proposed Capacity Scheduler to support fairness and priority sharing; both of them aim at achieving data locality and fairness in a large cluster.
LATE \cite{ZahariaM2008_LATE} improvs the speculative execution by accurately estimating the remaining time of tasks, which can better support Hadoop in heterogeneous environment.
HFS \cite{ZahariaM2010_DelayScheduling} uses delay scheduling to solve the conflict between data locality and fairness, and the job throughput can be improved by almost 2x.
All of these schedulers are suitable for the Hadoop reference model but not the decoupled model.

Hadoop has poor resource utilization due to the fixed number of map and reduce slots \cite{PoloJ2011_ResourceAware}.
Polo et al proposed a resource-aware scheduler that incorporates offline job profiling so that resource utilization  can be increased.
This work does not consider the decoupled Hadoop model, and the new Hadoop YARN supports flexible slot allocation; however, their job profiling can be applied to our system.
Moreover, instead of choosing the optimal slot number, our flow scheduling can utilize resource more efficiently because computing facilities can maintain high processing flow rate.

A scheduling problem can be solved as the network optimization problem.
Quincy \cite{IsardM2009_Quincy} adopts the min-cost flow network to achieve fair scheduling in a distributed commuting system.
Our flow scheduling is similar to this approach; however, our cost model is based on flow rate but not the data size that is required by a computing task.
We believe flow rate is a better choice because it can be a good indicator to determine the type of a task.
For example, a low flow rate task is a CPU-intensive task; however, the data size itself is not enough to determine the right task type.
CAM \cite{LiM2012_CAM} argues that the decoupled model is not suitable for virtualized clouds, and it attempts to co-allocate data with virtual machines.
CAM utilizes the network topology information and builds the min-cost flow network to reconcile data placement and VM placement.
