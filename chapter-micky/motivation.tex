\section{Why Collective Optimization}
\label{sec:motivation}

A cloud optimizer is often evaluated with search performance and measurement cost.

\begin{itemize}
\item \textbf{Search performance} is the measure of the quality of the found solutions by an optimizer.
For example, in searching for the most cost-effective configuration, an optimizer that finds a configuration that is only 10\% more expensive than the optimal is considered better than another optimizer
that can only find a configuration that yields 30\% more cost.
Therefore, we use normalized performance (to the optimal) for evaluation.

\item \textbf{Search cost}
is the total cost of running an optimizer.
An optimization process is expensive because it requires
to test a workload on some cloud configurations for deriving
the best choice.
We use the number of tests as the search cost (or measurement cost)
because it is an intuitive measure.
The amount of charge is another measure~\cite{Alipourfard2017}. 

\end{itemize}


There is always a trade-off between measurement cost and search performance.
The primary motivation for collective optimization is to reduce high measurement cost of optimizing multiple workloads.
If users demand strict search performance, they better turn to single-optimizers.
However, we argue that collective optimization is promising
because it achieves comparable or slightly worse search performance
while reducing measurement cost significantly.
In the following,
we discuss the benefits of having a collective optimizer.


\begin{itemize}
\item \textbf{Large scale cloud migration.}
Cloud computing is a cost-effective solution.
Enterprises are moving in-house applications to the cloud,
and need a quick way for large migration~\cite{khajeh2010cloud,sripanidkulchai2010clouds}.
Elaborate optimizers are expensive (in measurement cost) and time-consuming (in optimization process).

\item \textbf{Limited budgets.}
The single-optimizer such as \emph{CherryPick} and \emph{Scout} are effective and desirable for highly recurring workloads because the measurement cost can be amortized.
However, the number of budgets to run optimizers
does not increase linearly with the number of workloads.
To better support multiple workloads,
we need to reduce measurement cost while delivering comparable search performance.

\item \textbf{Expanding cloud portfolio.}
Cloud providers expand their cloud portfolio more than 20 times in a year~\cite{ec2history}.
Therefore, users have to rerun optimizers to update their configurations for all workloads.
Again, this is an expensive and time-consuming process.

\item \textbf{Seed cloud optimizers.}
All the cloud optimizers require initial measurements.
It is unclear how to determine the best starting points.
We aim to find the exemplar configurations,
which can be used as the starting points, thereby
reducing measurement cost.
The exemplar configuration can be used to seed singe-optimizers such as CherryPick and \scout, which will be discussed more in Section~\ref{sec:system}.

\end{itemize}

In summary, users would prefer collective optimization if search performance is comparable to single-optimizers while measurement cost can be reduced greatly.
