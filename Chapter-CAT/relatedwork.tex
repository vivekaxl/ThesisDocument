\section{State-of-the-Art Approaches}
\label{sec:cat::relatedwork}

\iffalse
Cloud providers recommend the choice of VM types~\cite{aws, google_rightsizing}.
However, it is too coarse grain and does not
apply to many workloads because
resource requirement is often opaque~\cite{Yadwadkar2017}.
This paper address the CAT problem, which is non-trivial
for the following reasons
~\cite{Venkataraman2016,Alipourfard2017,Yadwadkar2017,Hsu2018Arrow}.

\begin{itemize}
\item \textbf{Brute Force.}There are more than 100 cloud configurations.
Consequently, evaluating all the possible configurations
to find the best configuration is too expensive.

\item \textbf{Canonical Cloud Configuration.}
Each workload---application and input---has its own preferred choice
of cloud configuration.
Therefore, there is no one-size-fits-all configuration
~\cite{Alipourfard2017,Hsu2018Arrow,Venkataraman2016}. 

\item \textbf{Opaque Resource Requirements.}
Resource requirements to achieve a certain objective
(execution time or running cost)
for a specific workload are opaque~\cite{Yadwadkar2017}.

\item \textbf{Level playing field.}
While the execution time tends to decrease with a more powerful instance type,
the cost per unit time goes up, which compresses the running costs.
This creates a level playing field---several inferior configurations
in execution time are now competitive in running cost~\cite{Hsu2018Arrow}.
Consequently, it is harder to find the optimal cloud configuration
when the performance goal is reducing cost.

\end{itemize}

\fi



The CAT problem can be cast into 
a \emph{learning problem}---which uses elaborate offline evaluation
to generate a machine learning model that predicts the performance of workloads
~\cite{Venkataraman2016, Yadwadkar2017}
and an \emph{optimization problem}---which successively evaluates configurations
looking for one that is near optimal~\cite{Alipourfard2017, Hsu2018Arrow}.

Prediction, as proposed in PARIS~\cite{Yadwadkar2017},
is not reliable because of high variance in prediction results. 
Prediction accuracy heavily relies on feature selection, model selection,
and parameter tuning and the quality of data.
Sequential Model-Based Optimization (SMBO)~\cite{Dewancker2015}
is a search-based optimization method, which
does not require an accurate model but can have
a high evaluation cost (measured in terms of configurations evaluated).
Bayesian Optimization (used in \emph{CherryPick}) falls into
the SMBO class of algorithms.

The state of the art techniques such as
CherryPick~\cite{Alipourfard2017} and PARIS~\cite{Yadwadkar2017}
suffer from three major issues. 

\begin{itemize}
\item \textbf{Model accuracy.}
Prediction based approaches like PARIS build a model using measurements. The objective of such an approach is to use an accurate model to predict,
for example, execution time or running cost of workloads.
This method has two major weaknesses (i) building an accurate model requires more data---which in our setting is hard to come by, and (ii) the performance of the cloud environment is susceptible to performance variability---the data collected after running the workload might not reflect the true performance~\cite{tang2011impact}.
As shown in PARIS, the performance of batch-processing jobs is less predictable.
The inaccurate estimation of the execution time can be attributed to the non-linear relationship between resource and performance~\cite{Alipourfard2017}. 

\item \textbf{Cold-start}
Any SMBO method requires initial measurements to seed the search process.
The initial measurements are very crucial since
it determines the effectiveness of the search.
A poor seeding strategy can lead to wasted effort and
can yield a sub-optimal cloud configuration.
The effect of cold-start is more pronounced when 
the initial measurement cost cannot be amortized,
\eg{the search space is not large enough.}

\item \textbf{Fragility}
An SMBO method is fragile as it is overly sensitive to input parameters.
The success of \emph{CherryPick} on a given workload depends on the initial points used to seed the search and the choice of the kernel function used in the performance model (Gaussian Process Model).
Previous work shows that \emph{CherryPick}
sometimes fails to find near-optimal configurations and
incurs longer (than expected) search path~\cite{Hsu2018Arrow}.

\end{itemize}


Hyper-parameter tuning shares similarity with CAT.
For example, system and software performance is highly affected by configurations.
StarFish is an auto-tuning system for Hadoop applications~\cite{herodotou2011starfish}.
\emph{BestConfig} proposes the Divide and Diverge Sampling strategy along with the Recursive Bound and Search method
for turning software parameters~\cite{zhu2017bestconfig}.
A similar framework is also proposed to automate tuning system performance
of stream-processing systems~\cite{bilal2017towards}.
BOAT is a structured Bayesian Optimization-based framework for automatically tuning system performance
~\cite{Dalibard2017} which leverages contextual information.
Sampling techniques focus on reducing sampling cost while building accurate models to optimize software systems \cite{oh2017finding, nair2017}.
Parameter tuning is also an critical in machine learning~\cite{Dewancker2015,shahriari2016taking,Klein2017,golovin2017google}. 

The above methods focus on performance tuning for the same workload (or application)
on the same type of architecture.
It is still not clear how to leverage their approaches
to support different architectural configurations in cloud computing.


