\section{Problem Formalization}

Cloud architecture tuning (CAT) finds the best cloud configuration
for a workload ($w\in W$)---an application and its input.
\emph{IaaS} provides a set of computation, storage, and network resources.
For example, users have to determine the type and the number of VMs
to run a workload.
The search space ($S$) is a valid set of
architectural configurations for running the given workload.
The size of the search space is $|S|$ configurations
(the search space is the same for all workloads).
For a given workload $w$,
each configuration $s \in S$ has a corresponding cost measure $y=\phi(w, s)$.
The objective function, $\phi$, is user-specific.
In practice, cost measures can include
execution time, query throughput, running cost, etc. 

A cloud service provider presents its user with several choices of VM types ($\mathit{VM}$).
Let $\mathit{VM}_i$ indicate the $i^{th}$ VM type in the list of VMs.
%, which takes a value from a finite domain $Dom(\mathit{VM}_i)$.
In general, each VM type has distinct characteristics (such as memory size and core counts).
%$\mathit{VM}_i$ indicates the published characteristics of VMs (such as memory size and core counts).
%$\mathit{VM}_{ij}$ represents the $j^{th}$ characteristic of the $i^{th}$ VM type. The \textit{instance space} is thus $Dom(\mathit{VM}_1) \times Dom(\mathit{VM}_2) \times ... \times Dom(\mathit{VM}_n)$, which is the Cartesian product of the domains, where $n = \left\vert{\mathit{VM}}\right\vert$ is the number of VMs provided by the cloud service provider. 
When a workload ($w\in W$) is run on a VM ($\mathit{VM}_i$), the low-level metrics ($l_{i,w}\in L$) can be collected from the VM.
%These low-level metrics are denoted by $l_i$.
Each VM type ($\mathit{VM}$) has a corresponding performance measure $y\in Y$ (\eg{time or cost}).
We denote the performance measure associated with a given VM type and a workload by $y_{i,w}=f(\mathit{VM}_{i,w})$.
In this setting, $\mathit{VM}_{i,w}$ and $y_{i,w}$ are the independent and dependent variables, respectively.

An effective CAT method must
find (near) optimal configurations and
exhibit low \emph{search cost}.

\subsection*{Search performance}
Let $y^{*}$ be the optimum (minimum) cost for a workload, 
\emph{i.e.},
$y^* = \min_{\forall s \in S} \phi(w, s)$.
Let $\hat{y}$ be the best workload performance that a CAT optimizer finds.
The search performance of the CAT optimizer is $\hat{y}/y^{*}$
(lower the better).
A brute force approach finds the optimal configuration for a given workload.
Existing methods such as \emph{CherryPick} and \emph{Arrow} achieve
near-optimal search performance (\eg{$<1.1$}).

\subsection*{Search cost}
A CAT optimizer must evaluate a workload
on several architectural configurations to determine the best choice.
Such evaluation is generally expensive.
\emph{CherryPick} needs to evaluate at least three configurations
when running Bayesian Optimization and
\emph{PARIS} generates the fingerprint using two evaluations.
Let $E \subset S$ represent the search space that a CAT optimizer evaluated.
We define search cost as $|E|$---the number of evaluations needed to
select a configuration.
This measure is intuitive and effective to compare different CAT methods.
Other possible measures are
evaluation cost (dollars) and evaluation duration (time).

Our goal is to design a search method to:
\begin{enumerate}[leftmargin=*]
    \item \textit{Minimize} the performance difference between
the \emph{best} VM  ($\mathit{VM^*}$) (found by search) and the optimal VM  ($\mathit{VM}^{opt})$. We find $\mathit{VM^*}$ both in terms of \textit{execution time} and \textit{deployment cost};
    \item \textit{Minimize} search cost---the number of measurements required to find the (near) optimal configuration.
\end{enumerate}

We also look at other metrics for comparing CAT methods.
First, it is important to deliver \emph{reliable}
search performance and search cost across different workloads.
Some optimizers may encounter the fragility issue because
the search space is hard to model~\cite{Hsu2018Arrow}.
Second, a CAT method must be \emph{scalable}.
\emph{Ernest}, for example, must build a prediction model
for each VM family. 
Last, the optimizer must use a generic approach for adapting to
the rapid changes in cloud computing and software systems.
Exploiting workload information and system internals
can improve prediction performance but
makes a CAT method less applicable to distinct systems
~\cite{Wang2004,Venkataraman2016}.
