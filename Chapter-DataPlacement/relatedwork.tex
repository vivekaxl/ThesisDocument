% 
\section{Related Work}
\label{sec:related}

The data replication and placement issues have been extensively studied
in several research domains
including memory cache~\cite{Leff1993,Zaman2011},
distibuted storage systems~\cite{Lim2010},
NoSQL database~\cite{Rodrigues2013,Cruz2013,Corbett2013}
traditional retaional databases~\cite{Pavlo2012,Curino2010},
content delivery network (CDN)~\cite{Laoutaris2006}, etc.

Data replication is a common technique to increase
system performance and availability.
Many distributed systems use multiple replicas to
increase reliability and availability as well as improve system
performance~\cite{hbase,SageWeil2006Ceph}.
This chapter focuses on data replication and
its impact on system performance.
Data placement, on the other hand, determines the locations of data.
Apache Hadoop, for example, writes a replica locally and
two replicas in another rack to improve
system performance~\cite{Xie2010,Eltabakh2011}.
Our work makes contributions in three ways.
First, partition granularity determines the basic unit to calculate load.
Second, data replication is governed by anticipated workload.
Last, data placement chooses the slots on servers for maximizing
load balancing or exploiting cache locality.

Ceph is a distributed storage system
that supports configurable replication factors
for the placement group~\cite{SageWeil2006Ceph}.
Similarily, Apache Hadoop~\cite{hadoop} accepts separate replication factors
for different data chunks.
Optimizing the number of replications per object or data chunk
is a challenging task.
AptStore proposes the Popularity Prediction Algorithm (PPA) by
analyzing file system logs and adjusts replication factors accordingly
for files on Apache Hadoop~\cite{Krish2013}.
Scarlett combines offline log analysis and online prediction to
accurately estimate block popularity~\cite{Ananthanarayanan2011}.
Results show that replicating hotter data avoids bottlenecks,
thereby improving the performance of MapReduce jobs.
% These works greatly improve distributed storage performance but
% do not totally answer all the essential properties of
% data replication and placement.

The NoSQL database is a popular repository for large-scale data
because of they support extreme horizontal
scaling~\cite{Lakshman2010,Chang2008,hbase,mongodb}.
%\cmt{add more citations, eg, hbase, reddis,dynamodb, mongodb.}
Studies have shown that non-uniform query key
causes performance issues.
AUTOPLACER \cite{Rodrigues2013} optimizes the placement
for the top-k objects in the distributed key-value store.
To minimize the maintenance cost of large number of objects,
AUTOPLACER proposes the probabilistic data structure (PAA)
to reduce routing latency.
MET \cite{Cruz2013} is an elasticity framework that 
optimizes data placement to fit workload characteristics for HBase.
It is able to reconfigure HBase dynamically when an HBase cluster
requires expansion or contraction.
MET found that heterogeneous machines can easily lead to load imbalance.
To balance the load on each server, MET groups data partitions
according to their access pattern.
MET models the cost of placing data partitions on nodes, and
uses Longest Processing Time (LPT) to minimize the cost,
which maximizing load sharing among nodes.
%\cmt{what is makespan?}
The above implements a system that replicates and places data according to
data access pattern.
Our work explores how data replication and placement schemes
affect system performance under different workload distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% moved fig this from model.tex to make it float to first page of mode
%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Elastic storage dynamically expands or contracts a storage cluster.
The elastic controller is responsible for
provisioning resources and adjusting data replication and placement
for efficient storage configurations.
The SCADS director \cite{Trushkowsky2011} is an elastic controller
that automatically scales out a storage system to meet stringent
performance requirements. 
SCADS adopts a model-based approach to correlate request workload and
latency. 
It keeps track of the hottest bins (groups of data), and
dynamically moves these bins to different servers for balancing the load.
SCADS also creates performance models to predict workload and server capacity
for better estimating query latency.
Lim et al. propose a design of elastic storage\cite{Lim2010}.
Scaling a storage system poses challenges such as actuator delays,
measurement accuracy and interference with applications.
Their work puts emphasis on the elastic controller, whereas
we discover the relationship between
workload characteristics and data placement
when scaling a data-intensive application.

Sharding is a common technique to distribute load by
partitioning data across distributed server nodes.
More specifically, sharding is an approach that partitions data
according to keys, where partitions are placed
on different servers independently~\cite{Cattell2011}.
This horizontal scaling enables a distributed system to scale efficiently.
NoSQL databases, such as BigTable~\cite{Chang2008}, HBase~\cite{hbase}
and Cassandra~\cite{Lakshman2010}, rely on sharding
to efficiently balance load.

For relational databases, sharding is heavily used to support
a large volume of queries.
Schism uses graph-partitioning algorithms
to determine the optimal partitioning
for minimizing the cost of distributed transactions~\cite{Curino2010}.
The proposed fine-grained data partition minimizes
the required number of distributed transactions for database applications.
Pavlo \emph{et al}. develop a database tool that is able to generate
optimal database designs for parallel OLTP systems~\cite{Pavlo2012}.
They propose a new search algorithm for partitioning database and
create a cost model to minimize the coordination cost
while maximizing load balance.
Spanner is a semirelational database that
manages replicated data globally and performs resharding
at runtime for better load balancing~\cite{Corbett2013}.
Slicer is an auto-sharding service designed by Google~\cite{Adya2016}.
To leverage Slicer, users are required to
associate incoming requests with a key.
Slicer dynamically maps the key to a proper task
for maximizing load balance.
The above studies implement either
a sharding mechanism or design s generic sharding service,
while our work focuses on understanding the tradeoff
between different data placement schemes
when data partitions required proper placement to
meet workload distribution.
