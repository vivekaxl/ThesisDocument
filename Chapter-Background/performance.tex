\section{Performance Prediction and Optimization}

In this section, we describe the techniques to capture workloads and predict workload and system performance.
We also discuss using data-driven approach to optimizing performance.

\subsection{Workload Characterization}

Storage performance modeling has been extensively explored in prior work. 
Three common modeling techniques are analytical, simulation, and data-driven approaches \cite{Shriver1998, Kelly2004, Ardagna2014}.
The analytical model requires domain knowledge to manually identify the factors that affect performance\cite{Shriver1998, Kelly2004}.
Kelly et al. use a probability model to predict response time for an enterprise storage array.
Ruemmler et al. found that a disk is too complex to model with analytical methods and designed a disk simulator to characterize storage behavior \cite{Ruemmler1994}.
However, the simulation approach becomes inefficient when searching a large design space \cite{Kelly2004}.

Mesnier et al. \cite{Mesnier2007} propose a novel black-box approach that can describe the performance difference between two storage devices.
%It may be possible to borrow this idea and apply it to the unseen configuration scenarios as described in Section \ref{sec:unseen_configuration}.
With this approach, we can study the performance difference between two configurations and create a combined model with better prediction accuracy.
Bodik et al. propose an exploration policy for quick collection of essential data required to train a performance model \cite{Bodik2009}.
This policy can reduce the time required for offline and online model training.
Chen et al. propose SLA decomposition that combines profiling and queuing model to derive resource thresholds for meeting application SLA \cite{Chen2007}.


\subsection{Machine Learning}

The data-driven or the measurement-based approach
uses measurement data to derive a prediction model. 
Wang \etal adopt classification-and-regression-tree (CART) to predict the response time of a single disk and a disk array~\cite{Wang2004}.
The authors propose request-level and workload-level device models for different prediction granularities.
Yin et al. also use the regression tree to predict storage throughput and latency \cite{Yin2006}.
Their work mainly focuses on multiple workloads, and proposes a scalable model, by combining related workload features.
Noorshams et al. extensively analyze four different types of algorithms 
(including linear regression and CART models) and apply to IBM storage servers \cite{Noorshams2013}.
They also propose an optimization technique to search for parameters that can improve prediction accuracy.
Their proposed parameter optimization complements our work for improving prediction accuracy.
%To sum up, Inside-Out uses only low-level performance metrics and does not require workload profiles and storage configurations.
%Unlike these studies, Inside-Out is primarily designed for diverse storage services that need to be reconfigured frequently to meet users' demand. 
Machine learning has also been applied to performance modeling for virtual machines (VMs).
DeepDive uses the classification technique to detect performance anomaly among VMs \cite{Novakovic2013}.
In \cite{Kundu2010}, the authors apply regression and artificial neural network to model performance of a single VM.
%Our work focuses on performance prediction of a distributed storage system that includes multiple software and hardware entities.

\subsection{Low-Level Insights}
Low-level performance information is leveraged
to identify performance bottlenecks and to predict application performance.
DeepDive is designed to identify performance interference of co-existing VMs
~\cite{Novakovic2013}.
Wang \etal propose using the CART model to predict storage performance.
Their approach requires workload information, which may not be practical
for our problem setting.
Inside-out provides reliable performance prediction of distributed storage
service by using only low-level performance information~\cite{Hsu2016}.
The authors show that high-level performance can be accurately captured
by only the low-level metrics.
This accurate prediction model can be used to adjust resource allocation
for meeting performance objectives.


\subsection{Sequential Model-based Bayesian Optimization}
\label{sec:smbo}

Sequential model-based optimization (SMBO) is an optimization method that supports any black-box function~\cite{Dewancker2015}.
SMBO is naturally applicable to finding the best VM. 
SMBO iteratively measures solutions (VM types) to optimize for an objective (execution time or deployment cost).
A typical SMBO algorithm is described in Algorithm~\ref{alg:smbo}.
An SMBO algorithm requires 4 inputs namely, a cloud set up to run a workload ($f$), list of VM characteristics ($vm\in\mathit{VM}$) or instance space, an acquisition function ($S$), and a choice of surrogate model ($M$). SMBO starts with an initial sample of VMs (chosen randomly), which are then measured ($D$).
SMBO builds a surrogate or a machine learning model to estimate to predict workload performance.
This model is constructed using VM characteristics and the measured performance.
A VM is selected based on the surrogate model along with a predefined acquisition function.
The selected VM ($x_i$) is then measured ($f$).
The VM ($x_i$) along with performance ($y_i$) is then added to the already measured VMs ($D=\{(vm_1, y_1), (vm_2, y_2)\}$).
This process terminates after a stopping criterion is reached.

%We prefer SMBO because it is resistant to
%the shortcomings in the complex-model building method, and
%is suitable for optimizing any expensive black-box function.
% \textcolor{purple}{Although SMBO requires initial measurements, this overhead can be amortized in recurring workloads.}

\input{Chapter-Background/algorithms/smbo}



\subsection{Bayesian Optimization}
\label{sec:bo}

\bo (BO) follows the same formalism of sequential model-based optimization (SMBO).
Like SMBO, BO has two essential components namely a (probabilistic) regression model, and an acquisition function (refer to~\cite{shahriari2016taking} for more details.)
BO has been used as a drop-in replacement to standard techniques such as
random search, grid search and manual tuning in numerous domains such as hyperparameter tuning and software performance optimization~\cite{Dewancker2015, golovin2017google, nair2017, zuluaga2016varepsilon}.
To solve the CAT problem,
\emph{CherryPick} used BO to find the best VM for a specific workload~\cite{Alipourfard2017}.

In \bo, Gaussian Process is
the standard probabilistic model used for building the surrogate model.
Gaussian Process is a distribution over objective functions specified by a
mean function and covariance function.
Once a surrogate model is trained, it can be used to estimate performance (of a workload) on the unmeasured VM. The surrogate model returns distribution of the estimated performance associated with the VM (mean and variance). The next VM to measure is determined by an acquisition function. Common acquisition functions are Probability of Improvement (PI),
Expected Improvement (EI), and Gaussian Process Upper Confidence Bound (GP-UCB)~\cite{shahriari2016taking}.
Recently, the entropy search methods, backed by information theory, are promising alternatives~\cite{pmlr-v70-wang17e}.
In practice, EI is effective and used in \emph{CherryPick}.

% explain Mat√©rn kernel
An important component in Gaussian Process is the covariance kernel function, which is crucial for model effectiveness. Covariance kernel ensures that the prior, required for GP to be effective, is met. GP assumes smoothness, or in other words, the VMs which are closer to each other in instance space have similar performance.
% \footnote{Changing the Covariance kernel implies changing the prior.} 
This is particularly difficult in our problem setting, where a slight difference in the instance space can lead to significant differences in performance (cost or time). 
This indicates that before using BO (with GP as a surrogate model), a practitioner needs to choose a proper kernel function to ensure smoothness in the instance space.
Such a task is challenging and can affect the performance of BO. \emph{CherryPick} chooses the \emph{Mat\'ern 5/2} kernel function because it does not require strong smoothness, which are the cases for many real-world applications~\cite{Alipourfard2017,Yadwadkar2017}.


\subsection{Hyper-Parameter Tuning}

System and software performance is highly affected by configurations.
StarFish is an auto-tuning system for Hadoop applications~\cite{herodotou2011starfish}.
\emph{BestConfig} proposes the Divide and Diverge Sampling strategy along with the Recursive Bound and Search method
for turning software parameters~\cite{zhu2017bestconfig}.
Similar framework is also proposed to automate tuning system performance
of stream-processing systems~\cite{bilal2017towards}.
BOAT is a structured Bayesian Optimization-based framework for automatically tuning system performance
~\cite{Dalibard2017} which leverages contextual information.

\emph{BO4CO} uses Bayesian Optimization to continuously optimize system performance~\cite{jamshidi2016uncertainty}.
Similar to \emph{CherryPick}, \emph{BO4CO} leverages Bayesian Optimization but does not consider low-level metrics.
\emph{BOAT} is a structured Bayesian Optimization-based framework for automatically tuning system performance which leverages contextual information~\cite{Dalibard2017}.
\emph{BOAT} combines the parametric and non-parametric model
for better predicting the trend in system performance.
The idea behind their work and our work is very similar:
leveraging domain knowledge to enhance BO.
\emph{BOAT} optimizes software configurations while
\emph{Arrow} tunes architecture (virtual machines).
Parameter tuning is critical to machine learning application~\cite{Dewancker2015,shahriari2016taking,Klein2017,golovin2017google}.


Bilal \etal propose a framework to automate tuning system performance
of stream-processing systems~\cite{bilal2017towards}.
Their modified hill-climbing search with heuristic sampling
inspired by Latin Hypercube shortens the search process by two to five times.
The above methods reduce the search cost by a significant degree.
However, they focus on performance tuning for the same workload (or application)
on the same type of machine. It is not clear how to leverage their approaches to support different
machine configurations in cloud computing.
We, instead, find the best machine configuration for a given workload.

Sampling techniques focus on reducing sampling cost while building accurate models to optimize software systems \cite{oh2017finding, nair2017}.
The above methods reduce the search cost by a significant degree.
However, they focus on performance tuning for the same workload (or application) on the same type of machine. It is not clear how to leverage their approaches to support different
machine configurations in cloud computing. We, instead, find the best machine configuration for a given workload.


In the literature,
software configuration optimization~\cite{herodotou2011starfish,zhu2017bestconfig,bilal2017towards,Dalibard2017},
program parameter tuning~\cite{Klein2017,golovin2017google} and
sampling techniques~\cite{oh2017finding, nair2017} are active research directions.
They all focus on the same machine configuration.
It is not clear how to apply their approach directly to cloud environments, where
workloads perform very differently on distinct cloud configurations, \eg{VM types}.
