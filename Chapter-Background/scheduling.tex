\section{Resource and Job Scheduling}

Hadoop has the builtin FIFO scheduler, which allocates resources based on first-come-first-serve policy and data locality.
The Fair Scheduler was originally developed by Facebook with the objective of resource sharing, and Yahoo proposed Capacity Scheduler to support fairness and priority sharing; both of them aim at achieving data locality and fairness in a large cluster.
LATE \cite{ZahariaM2008_LATE} improvs the speculative execution by accurately estimating the remaining time of tasks, which can better support Hadoop in heterogeneous environment.
HFS \cite{ZahariaM2010_DelayScheduling} uses delay scheduling to solve the conflict between data locality and fairness, and the job throughput can be improved by almost 2x.
All of these schedulers are suitable for the Hadoop reference model but not the decoupled model.

Hadoop has poor resource utilization due to the fixed number of map and reduce slots \cite{PoloJ2011_ResourceAware}.
Polo et al proposed a resource-aware scheduler that incorporates offline job profiling so that resource utilization  can be increased.
This work does not consider the decoupled Hadoop model, and the new Hadoop YARN supports flexible slot allocation; however, their job profiling can be applied to our system.
Moreover, instead of choosing the optimal slot number, our flow scheduling can utilize resource more efficiently because computing facilities can maintain high processing flow rate.

A scheduling problem can be solved as the network optimization problem.
Quincy \cite{IsardM2009_Quincy} adopts the min-cost flow network to achieve fair scheduling in a distributed commuting system.
Our flow scheduling is similar to this approach; however, our cost model is based on flow rate but not the data size that is required by a computing task.
We believe flow rate is a better choice because it can be a good indicator to determine the type of a task.
For example, a low flow rate task is a CPU-intensive task; however, the data size itself is not enough to determine the right task type.
CAM \cite{LiM2012_CAM} argues that the decoupled model is not suitable for virtualized clouds, and it attempts to co-allocate data with virtual machines.
CAM utilizes the network topology information and builds the min-cost flow network to reconcile data placement and VM placement.