\section{Performance Optimization}

\subsection{Cloud Architecture Tuning}

\ernest exploits the internal structure of a workload
to predict its performance.
\ernest only needs smaller input for prediction.
This greatly reduces the search cost because
a workload can be experimentally tested on a smaller cluster.
However, \ernest is not scalable because
the prediction model is specific to a VM type.
\cherrypick implements an optimization engine that uses
Bayesian Optimization in searching for
the best configuration~\cite{Alipourfard2017}.
However, \cherrypick does not leverage
low-level information and uses Gaussian Process-based BO, which makes it fragile.
\paris shares the same goal with our work~\cite{Yadwadkar2017}.
It builds a comprehensive performance model from the large training dataset and uses it along with current measurements





\subsection{\micky}
The cloud computing optimizer determines the best cloud configuration (such as VM types and cluster sizes) for a given workload.
Users are looking for configurations that are
highly performing (\eg{the shortest execution time}) or
cost-effective (\eg{the cheapest operational cost}),
or meeting the trade-off between them~\cite{Alipourfard2017,Yadwadkar2017,Hsu2018Arrow}.
A poor choice, for example, can lead to
a 20 time slowdown or a 10 times increase in total cost~\cite{Hsu2018Arrow}.
Although cloud providers recommend the choice of VM types,
it is too coarse grain to be effective~\cite{aws, google_rightsizing}.
Besides, resource requirement for meeting a certain objective
is opaque~\cite{Yadwadkar2017}.
Previous attempts are listed as follows.

\textbf{Ernest}
exploits the internal structure of the workload to predict execution time of a workload~\cite{Venkataraman2016}. This significantly reduces measurement cost. However, \emph{Ernest} is not scalable because the prediction model is specific to a VM type.

\textbf{PARIS} uses historical data to build a learning model for predicting performance and cost of workloads on different VM types~\cite{Yadwadkar2017}.
Building an accurate model requires comprehensive training data to cover diverse workload characteristics.
Besides, it may suffer from high prediction error (as high as 50\%) in batch-processing workloads~\cite{Yadwadkar2017}. 

\textbf{CherryPick} uses Bayesian Optimization, which updates its beliefs (workload performance on configurations) and finds the best configuration sequentially~\cite{Alipourfard2017}.
Although Bayesian Optimization is powerful, it can be fragile when the search space is not well represented~\cite{Hsu2018Arrow}.


\textbf{Arrow} leverages low-level performance metrics to address the fragility issue in \textbf{CherryPick} due to insufficient representation in the search space and poor choices of the kernel function in Gaussian Process~\cite{Hsu2018Arrow}.

\textbf{Scout} uses historical data and leverages low-level performance metrics~\cite{Hsu2018Scout}.
This approach improves model accuracy, solves the cold-start issue and alleviates the fragility issue.


In the literature, software configuration optimization~\cite{herodotou2011starfish,zhu2017bestconfig,bilal2017towards,Dalibard2017}, program parameter tuning~\cite{Klein2017,golovin2017google} and sampling techniques~\cite{oh2017finding, nair2017} are active research directions.
They all focus on the same machine configuration.
It is not clear how to apply their approach directly to cloud environments, where
workloads perform very differently on distinct cloud configurations, \eg{VM types}.


One way to choose the best VM for a given workload is to build a complex prediction model from measurements as done in~\cite{Yadwadkar2017}.
However, this approach may encounter several obstacles.
First, the method assumes that data collection is free of noise.
However, this is not true in a cloud environment due to the sharing infrastructure, and therefore,
performance interference is unavoidable~\cite{Novakovic2013}. Second, a large amount of training data is required for building such complex models---which is not viable since each execution is expensive.
Moreover, even with the availability of data, performance predictability remains an issue.
For instance, \emph{PARIS} shows up to $50\%$ RMSE (Root Mean Squared Error) while predicting performance. 





